{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19b726fe-dfa0-4ed1-ac8d-f5f9e6053116",
   "metadata": {},
   "source": [
    "# PySpark data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fd4790b-c5ce-4f0e-b3b5-1b3afc54a9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c464c222-e3db-4984-b285-ca8f600aeaea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/04/26 23:31:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jupyter-ouseful-2dtemplat-2d-2dbinder-2dpyspark-2du9gz9x0l:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>big data course</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f6bd0e12e00>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "                        # Name to identify your experiment in the cluster's dashboard:\n",
    "                        .appName(\"big data course\")\n",
    "                        # Connect with the cluster's orchestrator:\n",
    "                        .master(\"local[*]\") # Cannot be \"local\" if you want to use your company's cluster.\n",
    "                        # Maximum memory any result dataframe will take up in driver memory:\n",
    "                        .config(\"spark.driver.maxResultSize\", \"4g\")\n",
    "                        # How much memory can be allocated to the driver (master/orchestrator):\n",
    "                        .config(\"spark.driver.memory\", \"1g\")\n",
    "                        # How much executors will be needed for the experiment:\n",
    "                        .config(\"spark.executor.instances\", \"5\")\n",
    "                        # Alternatively, allow spinning up more executors when there is more computation load, and discard them when less load:\n",
    "                        # .config(\"spark.dynamicAllocation.enabled\", True)\n",
    "                        # .config(\"spark.dynamicAllocation.minExecutors\", 1)\n",
    "                        # .config(\"spark.dynamic Allocation.maxExecutors\", 4)\n",
    "                        # How much memory can be allocated to each executor:\n",
    "                        .config(\"spark.executor.memory\", \"4g\")\n",
    "                        # How much CPU cores can be used to optimize parallellization within the executors (can be useful for shuffling data etc):\n",
    "                        #.config(\"spark.executor.cores\", 4)\n",
    "                        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "                        .getOrCreate()\n",
    ")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1f3afc-87d8-4479-a1d1-adaf9bd06ac6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Clarification on the \"Spark UI\" link that doesn't work here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6b32f71-f9ff-49c8-82e3-54f4632a6e4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://hub.gke2.mybinder.org/user/ouseful-templat--binder-pyspark-u9gz9x0l/'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binder_url = 'https://hub.mybinder.org' + os.environ['JUPYTERHUB_SERVICE_PREFIX']\n",
    "binder_url = \"https://hub.gke2.mybinder.org\" + os.environ[\"JUPYTERHUB_SERVICE_PREFIX\"]\n",
    "binder_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f66b686-f5e7-44a0-9f06-de477c53cdcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://hub.gke2.mybinder.org/user/ouseful-templat--binder-pyspark-u9gz9x0l/proxy/4040'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "external_spark_ui_url = binder_url + \"proxy/4040\"\n",
    "external_spark_ui_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d1a00d-b9b1-4e3f-ae2f-4fb93740087b",
   "metadata": {},
   "source": [
    "Normally, you should be able to visit the Spark UI when entering the above URL in a new browser tab. If a token is requested, you can copy-paste it from the command output of the next cell. But you'll probably get a HTTP error 4xx or 5xx.\n",
    "\n",
    "This *should* work, since the docker container of this binder project uses jupyter-server-proxy to make locally-listened ports available externally on the binder URL. See the documentation: https://jupyter-server-proxy.readthedocs.io/en/latest/arbitrary-ports-hosts.html\n",
    "\n",
    "It of course *doesn't* work though.\n",
    "There seems to be an issue with the proxying of the pyspark UI port 4040. We cannot navigate to it externally, but we can internally from within our container (see next cells). This is due to how the binder container is configured.\n",
    "\n",
    "So, let's settle with the fact that with this binder container, we will not be able to check the Spark UI when experimenting. Check that on your company's cluster, when moving from this toy environment towards \"the real thing\".\n",
    "Let's just use the binder container for now to play around with pySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5599c5a7-3328-4528-8d08-25ddc809f9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently running servers:\n",
      "http://jupyter-ouseful-2dtemplat-2d-2dbinder-2dpyspark-2du9gz9x0l:8888/user/ouseful-templat--binder-pyspark-u9gz9x0l/?token=-vnIMZ4cSOStJX-kBRUmWA :: /home/jovyan\n"
     ]
    }
   ],
   "source": [
    "! jupyter server list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3e5a387-8d2d-4309-b2aa-9ae854934c87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://jupyter-ouseful-2dtemplat-2d-2dbinder-2dpyspark-2du9gz9x0l:4040'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# url used internally within this binder's docker container:\n",
    "spark.sparkContext.uiWebUrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "06aecc93-a1fc-4047-a03c-3a5226f072aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://jupyter-ouseful-2dtemplat-2d-2dbinder-2dpyspark-2du9gz9x0l:4040/jobs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html><html>\n",
       "      <head>\n",
       "        <meta http-equiv=\"Content-type\" content=\"text/html; charset=utf-8\"/><meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"/><link rel=\"stylesheet\" href=\"/static/bootstrap.min.css\" type=\"text/css\"/><link rel=\"stylesheet\" href=\"/static/vis-timeline-graph2d.min.css\" type=\"text/css\"/><link rel=\"stylesheet\" href=\"/static/webui.css\" type=\"text/css\"/><link rel=\"stylesheet\" href=\"/static/timeline-view.css\" type=\"text/css\"/><script src=\"/static/sorttable.js\"></script><script src=\"/static/jquery-3.5.1.min.js\"></script><script src=\"/static/vis-timeline-graph2d.min.js\"></script><script src=\"/static/bootstrap.bundle.min.js\"></script><script src=\"/static/initialize-tooltips.js\"></script><script src=\"/static/table.js\"></script><script src=\"/static/timeline-view.js\"></script><script src=\"/static/log-view.js\"></script><script src=\"/static/webui.js\"></script><script>setUIRoot('')</script>\n",
       "        <script>setAppBasePath('')</script>\n",
       "        \n",
       "        \n",
       "        <link rel=\"shortcut icon\" href=\"/static/spark-logo-77x50px-hd.png\"></link>\n",
       "        <title>big data course - Spark Jobs</title>\n",
       "      </head>\n",
       "      <body>\n",
       "        <nav class=\"navbar navbar-expand-md navbar-light bg-light mb-4\">\n",
       "          <div class=\"navbar-header\">\n",
       "            <div class=\"navbar-brand\">\n",
       "              <a href=\"/\">\n",
       "                <img src=\"/static/spark-logo-77x50px-hd.png\"/>\n",
       "                <span class=\"version\">3.4.0</span>\n",
       "              </a>\n",
       "            </div>\n",
       "          </div>\n",
       "          <button class=\"navbar-toggler\" type=\"button\" data-toggle=\"collapse\" data-target=\"#navbarCollapse\" aria-controls=\"navbarCollapse\" aria-expanded=\"false\" aria-label=\"Toggle navigation\">\n",
       "            <span class=\"navbar-toggler-icon\"></span>\n",
       "          </button>\n",
       "          <div class=\"collapse navbar-collapse\" id=\"navbarCollapse\">\n",
       "            <ul class=\"navbar-nav mr-auto\"><li class=\"nav-item active\">\n",
       "        <a class=\"nav-link\" href=\"/jobs/\">Jobs</a>\n",
       "      </li><li class=\"nav-item\">\n",
       "        <a class=\"nav-link\" href=\"/stages/\">Stages</a>\n",
       "      </li><li class=\"nav-item\">\n",
       "        <a class=\"nav-link\" href=\"/storage/\">Storage</a>\n",
       "      </li><li class=\"nav-item\">\n",
       "        <a class=\"nav-link\" href=\"/environment/\">Environment</a>\n",
       "      </li><li class=\"nav-item\">\n",
       "        <a class=\"nav-link\" href=\"/executors/\">Executors</a>\n",
       "      </li><li class=\"nav-item\">\n",
       "        <a class=\"nav-link\" href=\"/SQL/\">SQL / DataFrame</a>\n",
       "      </li></ul>\n",
       "            <span class=\"navbar-text navbar-right d-none d-md-block\">\n",
       "              <strong title=\"big data course\" class=\"text-nowrap\">big data course</strong>\n",
       "              <span class=\"text-nowrap\">application UI</span>\n",
       "            </span>\n",
       "          </div>\n",
       "        </nav>\n",
       "        <div class=\"container-fluid\">\n",
       "          <div class=\"row\">\n",
       "            <div class=\"col-12\">\n",
       "              <h3 style=\"vertical-align: bottom; white-space: nowrap; overflow: hidden;\n",
       "                text-overflow: ellipsis;\">\n",
       "                Spark Jobs\n",
       "                <sup>\n",
       "      (<a data-toggle=\"tooltip\" data-placement=\"top\" title=\"A job is triggered by an action, like count() or saveAsTextFile(). Click on a job to see information about the stages of tasks inside it.\">?</a>)\n",
       "    </sup>\n",
       "              </h3>\n",
       "            </div>\n",
       "          </div>\n",
       "          <div class=\"row\">\n",
       "            <div class=\"col-12\">\n",
       "              <div>\n",
       "        <ul class=\"list-unstyled\">\n",
       "          <li>\n",
       "\n",
       "            <strong>User:</strong>\n",
       "            jovyan\n",
       "          </li>\n",
       "          <li>\n",
       "            <strong>Total Uptime:</strong>\n",
       "            21 min\n",
       "          </li>\n",
       "          <li>\n",
       "            <strong>Scheduling Mode: </strong>\n",
       "            FIFO\n",
       "          </li>\n",
       "          \n",
       "          \n",
       "          \n",
       "        </ul>\n",
       "      </div><span class=\"expand-application-timeline\">\n",
       "      <span class=\"expand-application-timeline-arrow arrow-closed\"></span>\n",
       "      <a data-toggle=\"tooltip\" title=\"Shows when jobs started and ended and when executors joined or left. Drag to scroll.\n",
       "       Click Enable Zooming and use mouse wheel to zoom in/out.\" data-placement=\"top\">\n",
       "        Event Timeline\n",
       "      </a>\n",
       "    </span><div id=\"application-timeline\" class=\"collapsed\">\n",
       "      \n",
       "      \n",
       "      <div class=\"control-panel\">\n",
       "        <div id=\"application-timeline-zoom-lock\">\n",
       "          <input type=\"checkbox\"></input>\n",
       "          <span>Enable zooming</span>\n",
       "        </div>\n",
       "      </div>\n",
       "    </div><script type=\"text/javascript\">\n",
       "      drawApplicationTimeline(\n",
       "[\n",
       "  {\n",
       "    'id': 'executors',\n",
       "    'content': '<div>Executors</div><div class=\"legend-area\"><svg width=\"150px\" height=\"55px\">      <rect class=\"executor-added-legend\" x=\"5px\" y=\"5px\" width=\"20px\" height=\"15px\" rx=\"2px\" ry=\"2px\"></rect>      <text x=\"35px\" y=\"17px\">Added</text>      <rect class=\"executor-removed-legend\" x=\"5px\" y=\"30px\" width=\"20px\" height=\"15px\" rx=\"2px\" ry=\"2px\"></rect>      <text x=\"35px\" y=\"42px\">Removed</text>    </svg></div>',\n",
       "  },\n",
       "  {\n",
       "    'id': 'jobs',\n",
       "    'content': '<div>Jobs</div><div class=\"legend-area\"><svg width=\"150px\" height=\"85px\">      <rect class=\"succeeded-job-legend\" x=\"5px\" y=\"5px\" width=\"20px\" height=\"15px\" rx=\"2px\" ry=\"2px\"></rect>      <text x=\"35px\" y=\"17px\">Succeeded</text>      <rect class=\"failed-job-legend\" x=\"5px\" y=\"30px\" width=\"20px\" height=\"15px\" rx=\"2px\" ry=\"2px\"></rect>      <text x=\"35px\" y=\"42px\">Failed</text>      <rect class=\"running-job-legend\" x=\"5px\" y=\"55px\" width=\"20px\" height=\"15px\" rx=\"2px\" ry=\"2px\"></rect>      <text x=\"35px\" y=\"67px\">Running</text>    </svg></div>',\n",
       "  }\n",
       "]\n",
       "        ,[\n",
       "{\n",
       "  'className': 'executor added',\n",
       "  'group': 'executors',\n",
       "  'start': new Date(1682549184907),\n",
       "  'content': '<div class=\"executor-event-content\"' +\n",
       "    'data-toggle=\"tooltip\" data-placement=\"top\"' +\n",
       "    'data-title=\"Executor driver<br>' +\n",
       "    'Added at 2023/04/26 22:46:24\"' +\n",
       "    'data-html=\"true\">Executor driver added</div>'\n",
       "}\n",
       "         ], 1682549184792, 0);\n",
       "    </script>\n",
       "            </div>\n",
       "          </div>\n",
       "        </div>\n",
       "      </body>\n",
       "    </html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This isn't great, but with this hack we can see that the Spark UI *is* live:\n",
    "import requests\n",
    "from IPython.display import HTML\n",
    "\n",
    "def render_local_spark_ui(subpage=\"jobs\"):\n",
    "    subpage_url = spark.sparkContext.uiWebUrl + (\"\" if subpage is None else \"/\" + subpage)\n",
    "    print(subpage_url)\n",
    "    response = requests.get(subpage_url)\n",
    "    return HTML(data=bytes.decode(response.content))\n",
    "    \n",
    "render_local_spark_ui(subpage=\"jobs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a002b835-272e-4323-b108-9d8a6e10a233",
   "metadata": {},
   "source": [
    "## Exploratory data analysis\n",
    "\n",
    "On a small sample of the popular flights dataset: https://github.com/ozlerhakan/datacamp/blob/master/Introduction%20to%20PySpark/flights_small.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a94b675-98e4-4f3d-95b4-4e8495f040c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- dep_time: string (nullable = true)\n",
      " |-- dep_delay: string (nullable = true)\n",
      " |-- arr_time: string (nullable = true)\n",
      " |-- arr_delay: string (nullable = true)\n",
      " |-- carrier: string (nullable = true)\n",
      " |-- tailnum: string (nullable = true)\n",
      " |-- flight: integer (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- dest: string (nullable = true)\n",
      " |-- air_time: string (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      " |-- hour: string (nullable = true)\n",
      " |-- minute: string (nullable = true)\n",
      "\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|2014|   12|  8|     658|       -7|     935|       -5|     VX| N846VA|  1780|   SEA| LAX|     132|     954|   6|    58|\n",
      "|2014|    1| 22|    1040|        5|    1505|        5|     AS| N559AS|   851|   SEA| HNL|     360|    2677|  10|    40|\n",
      "|2014|    3|  9|    1443|       -2|    1652|        2|     VX| N847VA|   755|   SEA| SFO|     111|     679|  14|    43|\n",
      "|2014|    4|  9|    1705|       45|    1839|       34|     WN| N360SW|   344|   PDX| SJC|      83|     569|  17|     5|\n",
      "|2014|    3|  9|     754|       -1|    1015|        1|     AS| N612AS|   522|   SEA| BUR|     127|     937|   7|    54|\n",
      "|2014|    1| 15|    1037|        7|    1352|        2|     WN| N646SW|    48|   PDX| DEN|     121|     991|  10|    37|\n",
      "|2014|    7|  2|     847|       42|    1041|       51|     WN| N422WN|  1520|   PDX| OAK|      90|     543|   8|    47|\n",
      "|2014|    5| 12|    1655|       -5|    1842|      -18|     VX| N361VA|   755|   SEA| SFO|      98|     679|  16|    55|\n",
      "|2014|    4| 19|    1236|       -4|    1508|       -7|     AS| N309AS|   490|   SEA| SAN|     135|    1050|  12|    36|\n",
      "|2014|   11| 19|    1812|       -3|    2352|       -4|     AS| N564AS|    26|   SEA| ORD|     198|    1721|  18|    12|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"flights_small.csv\", header=True, inferSchema=True) # if inferSchema=True is not set, all columns are just string.\n",
    "df.printSchema()\n",
    "df.show(10) # if lots of columns, this plots nicer: .limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc69f6f7-89b3-489a-8b7c-2a911877a1c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869cd1a1-5dc0-4976-972f-7a00b7f6bb56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a33ad6-244e-4365-9b86-322c5df77f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet(\"flights_results.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "802cc119-5661-4f88-8e78-86eead8d053c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
